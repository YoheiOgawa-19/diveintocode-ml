{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tensor1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dcb4c62629d7f39ea82b16f1b71ddb4e62ad343aa7915e53b2cf1fe65a934dad"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Sprint ディープラーニングフレームワーク1\n",
    "\n",
    "### 2.コードリーディング\n",
    "\n",
    "\n",
    "TensorFlowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n",
    "\n",
    "tf.keras や tf.estimator などの高レベルAPIは使用していません。低レベルなところから見ていくことにします。\n",
    "\n",
    "#### 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "（例）\n",
    "\n",
    "重みを初期化する必要があった\n",
    "エポックのループが必要だった\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 必要なもの　　\n",
    "・　pythonの知識  \n",
    "・　numpyの知識  \n",
    "・　フォワードプロパゲーションとバックプロパゲーションの考え方  \n",
    "・　活性化関数の種類と知識  \n",
    "・　アルゴリズムの種類  \n",
    "・　損失関数の知識  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
    "\n",
    "Iris Species\n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。  \n",
    "\n",
    "Iris-versicolor\n",
    "Iris-virginica"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n",
    "《サンプルコード》\n",
    "\n",
    "＊TensorFlow バージョン 1.5 から 1.14 までで動作を確認済みです。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/a100/.pyenv/versions/anaconda3-5.1.0/envs/tensor1/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "ipykernel_launcher:49: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Epoch 0, loss : 5.4193, val_loss : 57.2231, acc : 0.375\n",
      "Epoch 1, loss : 3.5689, val_loss : 34.6412, acc : 0.375\n",
      "Epoch 2, loss : 2.1442, val_loss : 15.9005, acc : 0.188\n",
      "Epoch 3, loss : 1.7802, val_loss : 11.7099, acc : 0.062\n",
      "Epoch 4, loss : 1.4782, val_loss : 11.4397, acc : 0.188\n",
      "Epoch 5, loss : 1.2006, val_loss : 12.0541, acc : 0.250\n",
      "Epoch 6, loss : 1.0606, val_loss : 8.0987, acc : 0.188\n",
      "Epoch 7, loss : 0.9051, val_loss : 5.9181, acc : 0.312\n",
      "Epoch 8, loss : 0.7778, val_loss : 5.5638, acc : 0.312\n",
      "Epoch 9, loss : 0.6314, val_loss : 4.3611, acc : 0.312\n",
      "Epoch 10, loss : 0.4857, val_loss : 2.7496, acc : 0.438\n",
      "Epoch 11, loss : 0.3648, val_loss : 2.1538, acc : 0.438\n",
      "Epoch 12, loss : 0.2902, val_loss : 1.5546, acc : 0.438\n",
      "Epoch 13, loss : 0.2336, val_loss : 1.0378, acc : 0.562\n",
      "Epoch 14, loss : 0.1847, val_loss : 0.9604, acc : 0.688\n",
      "Epoch 15, loss : 0.1568, val_loss : 0.7830, acc : 0.688\n",
      "Epoch 16, loss : 0.1283, val_loss : 0.5272, acc : 0.812\n",
      "Epoch 17, loss : 0.1076, val_loss : 0.3977, acc : 0.875\n",
      "Epoch 18, loss : 0.0932, val_loss : 0.2476, acc : 0.875\n",
      "Epoch 19, loss : 0.0830, val_loss : 0.1727, acc : 0.938\n",
      "Epoch 20, loss : 0.0759, val_loss : 0.1069, acc : 0.938\n",
      "Epoch 21, loss : 0.0700, val_loss : 0.0713, acc : 1.000\n",
      "Epoch 22, loss : 0.0653, val_loss : 0.0530, acc : 1.000\n",
      "Epoch 23, loss : 0.0614, val_loss : 0.0443, acc : 1.000\n",
      "Epoch 24, loss : 0.0583, val_loss : 0.0419, acc : 1.000\n",
      "Epoch 25, loss : 0.0559, val_loss : 0.0413, acc : 1.000\n",
      "Epoch 26, loss : 0.0540, val_loss : 0.0416, acc : 1.000\n",
      "Epoch 27, loss : 0.0524, val_loss : 0.0417, acc : 1.000\n",
      "Epoch 28, loss : 0.0507, val_loss : 0.0417, acc : 1.000\n",
      "Epoch 29, loss : 0.0492, val_loss : 0.0412, acc : 1.000\n",
      "Epoch 30, loss : 0.0476, val_loss : 0.0403, acc : 1.000\n",
      "Epoch 31, loss : 0.0455, val_loss : 0.0394, acc : 1.000\n",
      "Epoch 32, loss : 0.0425, val_loss : 0.0329, acc : 1.000\n",
      "Epoch 33, loss : 0.0391, val_loss : 0.0293, acc : 1.000\n",
      "Epoch 34, loss : 0.0356, val_loss : 0.0289, acc : 1.000\n",
      "Epoch 35, loss : 0.0319, val_loss : 0.0310, acc : 1.000\n",
      "Epoch 36, loss : 0.0281, val_loss : 0.0372, acc : 1.000\n",
      "Epoch 37, loss : 0.0242, val_loss : 0.0494, acc : 1.000\n",
      "Epoch 38, loss : 0.0213, val_loss : 0.0600, acc : 1.000\n",
      "Epoch 39, loss : 0.0190, val_loss : 0.0637, acc : 1.000\n",
      "Epoch 40, loss : 0.0170, val_loss : 0.0598, acc : 1.000\n",
      "Epoch 41, loss : 0.0154, val_loss : 0.0547, acc : 1.000\n",
      "Epoch 42, loss : 0.0140, val_loss : 0.0473, acc : 1.000\n",
      "Epoch 43, loss : 0.0132, val_loss : 0.0516, acc : 1.000\n",
      "Epoch 44, loss : 0.0123, val_loss : 0.0554, acc : 1.000\n",
      "Epoch 45, loss : 0.0113, val_loss : 0.0501, acc : 1.000\n",
      "Epoch 46, loss : 0.0108, val_loss : 0.0510, acc : 1.000\n",
      "Epoch 47, loss : 0.0104, val_loss : 0.0549, acc : 1.000\n",
      "Epoch 48, loss : 0.0097, val_loss : 0.0518, acc : 1.000\n",
      "Epoch 49, loss : 0.0095, val_loss : 0.0531, acc : 1.000\n",
      "Epoch 50, loss : 0.0091, val_loss : 0.0561, acc : 1.000\n",
      "Epoch 51, loss : 0.0087, val_loss : 0.0541, acc : 1.000\n",
      "Epoch 52, loss : 0.0085, val_loss : 0.0568, acc : 1.000\n",
      "Epoch 53, loss : 0.0082, val_loss : 0.0581, acc : 1.000\n",
      "Epoch 54, loss : 0.0079, val_loss : 0.0567, acc : 1.000\n",
      "Epoch 55, loss : 0.0077, val_loss : 0.0585, acc : 1.000\n",
      "Epoch 56, loss : 0.0074, val_loss : 0.0592, acc : 1.000\n",
      "Epoch 57, loss : 0.0072, val_loss : 0.0592, acc : 0.938\n",
      "Epoch 58, loss : 0.0069, val_loss : 0.0599, acc : 0.938\n",
      "Epoch 59, loss : 0.0067, val_loss : 0.0595, acc : 0.938\n",
      "Epoch 60, loss : 0.0065, val_loss : 0.0614, acc : 0.938\n",
      "Epoch 61, loss : 0.0063, val_loss : 0.0609, acc : 0.938\n",
      "Epoch 62, loss : 0.0061, val_loss : 0.0619, acc : 0.938\n",
      "Epoch 63, loss : 0.0059, val_loss : 0.0614, acc : 0.938\n",
      "Epoch 64, loss : 0.0057, val_loss : 0.0627, acc : 0.938\n",
      "Epoch 65, loss : 0.0055, val_loss : 0.0625, acc : 0.938\n",
      "Epoch 66, loss : 0.0054, val_loss : 0.0632, acc : 0.938\n",
      "Epoch 67, loss : 0.0052, val_loss : 0.0624, acc : 0.938\n",
      "Epoch 68, loss : 0.0051, val_loss : 0.0638, acc : 0.938\n",
      "Epoch 69, loss : 0.0049, val_loss : 0.0629, acc : 0.938\n",
      "Epoch 70, loss : 0.0048, val_loss : 0.0639, acc : 0.938\n",
      "Epoch 71, loss : 0.0046, val_loss : 0.0627, acc : 0.938\n",
      "Epoch 72, loss : 0.0045, val_loss : 0.0639, acc : 0.938\n",
      "Epoch 73, loss : 0.0043, val_loss : 0.0628, acc : 0.938\n",
      "Epoch 74, loss : 0.0042, val_loss : 0.0638, acc : 0.938\n",
      "Epoch 75, loss : 0.0041, val_loss : 0.0634, acc : 0.938\n",
      "Epoch 76, loss : 0.0040, val_loss : 0.0639, acc : 0.938\n",
      "Epoch 77, loss : 0.0039, val_loss : 0.0638, acc : 0.938\n",
      "Epoch 78, loss : 0.0038, val_loss : 0.0639, acc : 0.938\n",
      "Epoch 79, loss : 0.0037, val_loss : 0.0638, acc : 0.938\n",
      "Epoch 80, loss : 0.0036, val_loss : 0.0632, acc : 0.938\n",
      "Epoch 81, loss : 0.0035, val_loss : 0.0637, acc : 0.938\n",
      "Epoch 82, loss : 0.0034, val_loss : 0.0627, acc : 0.938\n",
      "Epoch 83, loss : 0.0034, val_loss : 0.0632, acc : 0.938\n",
      "Epoch 84, loss : 0.0033, val_loss : 0.0623, acc : 0.938\n",
      "Epoch 85, loss : 0.0032, val_loss : 0.0625, acc : 0.938\n",
      "Epoch 86, loss : 0.0031, val_loss : 0.0618, acc : 0.938\n",
      "Epoch 87, loss : 0.0031, val_loss : 0.0619, acc : 0.938\n",
      "Epoch 88, loss : 0.0030, val_loss : 0.0613, acc : 0.938\n",
      "Epoch 89, loss : 0.0029, val_loss : 0.0612, acc : 0.938\n",
      "Epoch 90, loss : 0.0029, val_loss : 0.0607, acc : 0.938\n",
      "Epoch 91, loss : 0.0028, val_loss : 0.0605, acc : 0.938\n",
      "Epoch 92, loss : 0.0028, val_loss : 0.0601, acc : 0.938\n",
      "Epoch 93, loss : 0.0027, val_loss : 0.0599, acc : 0.938\n",
      "Epoch 94, loss : 0.0027, val_loss : 0.0595, acc : 0.938\n",
      "Epoch 95, loss : 0.0026, val_loss : 0.0592, acc : 0.938\n",
      "Epoch 96, loss : 0.0026, val_loss : 0.0588, acc : 0.938\n",
      "Epoch 97, loss : 0.0025, val_loss : 0.0584, acc : 0.938\n",
      "Epoch 98, loss : 0.0025, val_loss : 0.0581, acc : 0.938\n",
      "Epoch 99, loss : 0.0024, val_loss : 0.0577, acc : 0.938\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['Species'] = iris.target\n",
    "df.loc[df['Species'] == 0, 'Species'] = \"iris-setosa\"\n",
    "df.loc[df['Species'] == 1, 'Species'] = \"Iris-versicolor\"\n",
    "df.loc[df['Species'] == 2, 'Species'] = \"Iris-virginica\"\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# ラベルを数値に変換\n",
    "y[y == \"Iris-versicolor\"] = 0\n",
    "y[y == \"Iris-virginica\"] = 1\n",
    "y = y.astype(np.int64)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "source": [
    "・　pythonの知識\n",
    "-> 言わずもが  \n",
    "\n",
    "・　numpyの知識\n",
    "-> ndarray型に変換して処理を行なっている  \n",
    "\n",
    "・　フォワードプロパゲーションとバックプロパゲーションの考え方  \n",
    "・　活性化関数の種類と知識  \n",
    "・　アルゴリズムの種類  \n",
    "->  　# 最適化手法    optimizer = tf.train.AdamOptimizer　learning_rate=learning_rate) train_op = optimizer.minimize(loss_op) \n",
    "にて行なわれている。  \n",
    "\n",
    "・　損失関数の知識  \n",
    " ->       # 目的関数\n",
    "        loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.他のデータセットへの適用\n",
    "\n",
    "\n",
    "これまで扱ってきた小さなデータセットが他にもいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
    "\n",
    "Iris（3種類全ての目的変数を使用）\n",
    "House Prices\n",
    "\n",
    "どのデータセットも train, val, test の3種類に分けて使用してください。\n",
    "\n",
    "#### 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "\n",
    "Iris Species\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "《ヒント》\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。\n",
    "\n",
    "1\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "1\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "[tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "[tf.math.sign  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/math/sign)\n",
    "\n",
    "＊tf.sign と tf.math.sign は同じ関数です。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([1., 0., 0.], dtype=float32), (150, 3))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y[5,:],y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_2:0' shape=() dtype=float32>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax:0' shape=(3,) dtype=int64>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "tf.argmax(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, loss : 15.7031, val_loss : 117.8337, acc : 0.208\n",
      "Epoch 1, loss : 9.6233, val_loss : 70.9387, acc : 0.000\n",
      "Epoch 2, loss : 5.0294, val_loss : 37.2532, acc : 0.250\n",
      "Epoch 3, loss : 3.1495, val_loss : 28.1203, acc : 0.417\n",
      "Epoch 4, loss : 2.1973, val_loss : 16.0404, acc : 0.542\n",
      "Epoch 5, loss : 1.1483, val_loss : 5.7439, acc : 0.375\n",
      "Epoch 6, loss : 0.3438, val_loss : 0.4589, acc : 0.958\n",
      "Epoch 7, loss : 0.0808, val_loss : 0.2685, acc : 0.917\n",
      "Epoch 8, loss : 0.0299, val_loss : 0.5311, acc : 0.917\n",
      "Epoch 9, loss : 0.0294, val_loss : 0.3102, acc : 0.958\n",
      "Epoch 10, loss : 0.0227, val_loss : 0.4524, acc : 0.917\n",
      "Epoch 11, loss : 0.0238, val_loss : 0.3613, acc : 0.958\n",
      "Epoch 12, loss : 0.0209, val_loss : 0.4284, acc : 0.917\n",
      "Epoch 13, loss : 0.0207, val_loss : 0.3992, acc : 0.958\n",
      "Epoch 14, loss : 0.0189, val_loss : 0.4417, acc : 0.917\n",
      "Epoch 15, loss : 0.0182, val_loss : 0.4344, acc : 0.917\n",
      "Epoch 16, loss : 0.0168, val_loss : 0.4737, acc : 0.917\n",
      "Epoch 17, loss : 0.0160, val_loss : 0.4700, acc : 0.917\n",
      "Epoch 18, loss : 0.0146, val_loss : 0.5120, acc : 0.917\n",
      "Epoch 19, loss : 0.0139, val_loss : 0.4951, acc : 0.917\n",
      "Epoch 20, loss : 0.0124, val_loss : 0.5528, acc : 0.917\n",
      "Epoch 21, loss : 0.0121, val_loss : 0.4988, acc : 0.917\n",
      "Epoch 22, loss : 0.0101, val_loss : 0.6124, acc : 0.917\n",
      "Epoch 23, loss : 0.0108, val_loss : 0.4192, acc : 0.917\n",
      "Epoch 24, loss : 0.0075, val_loss : 0.7399, acc : 0.917\n",
      "Epoch 25, loss : 0.0112, val_loss : 0.2160, acc : 0.958\n",
      "Epoch 26, loss : 0.0085, val_loss : 0.8866, acc : 0.917\n",
      "Epoch 27, loss : 0.0118, val_loss : 0.1768, acc : 0.917\n",
      "Epoch 28, loss : 0.0125, val_loss : 1.0521, acc : 0.917\n",
      "Epoch 29, loss : 0.0149, val_loss : 0.1824, acc : 0.958\n",
      "Epoch 30, loss : 0.0134, val_loss : 1.0866, acc : 0.917\n",
      "Epoch 31, loss : 0.0157, val_loss : 0.1802, acc : 0.958\n",
      "Epoch 32, loss : 0.0145, val_loss : 1.1137, acc : 0.917\n",
      "Epoch 33, loss : 0.0164, val_loss : 0.1747, acc : 0.917\n",
      "Epoch 34, loss : 0.0163, val_loss : 1.1353, acc : 0.917\n",
      "Epoch 35, loss : 0.0177, val_loss : 0.1807, acc : 0.917\n",
      "Epoch 36, loss : 0.0203, val_loss : 1.1727, acc : 0.917\n",
      "Epoch 37, loss : 0.0203, val_loss : 0.1936, acc : 0.875\n",
      "Epoch 38, loss : 0.0234, val_loss : 1.2858, acc : 0.917\n",
      "Epoch 39, loss : 0.0203, val_loss : 0.2283, acc : 0.875\n",
      "Epoch 40, loss : 0.0264, val_loss : 1.4205, acc : 0.917\n",
      "Epoch 41, loss : 0.0196, val_loss : 0.2532, acc : 0.833\n",
      "Epoch 42, loss : 0.0259, val_loss : 1.4145, acc : 0.917\n",
      "Epoch 43, loss : 0.0195, val_loss : 0.2765, acc : 0.833\n",
      "Epoch 44, loss : 0.0255, val_loss : 1.3666, acc : 0.917\n",
      "Epoch 45, loss : 0.0169, val_loss : 0.2232, acc : 0.875\n",
      "Epoch 46, loss : 0.0234, val_loss : 1.3097, acc : 0.917\n",
      "Epoch 47, loss : 0.0181, val_loss : 0.2642, acc : 0.833\n",
      "Epoch 48, loss : 0.0252, val_loss : 1.3783, acc : 0.917\n",
      "Epoch 49, loss : 0.0166, val_loss : 0.2464, acc : 0.833\n",
      "Epoch 50, loss : 0.0245, val_loss : 1.3989, acc : 0.917\n",
      "Epoch 51, loss : 0.0187, val_loss : 0.3081, acc : 0.833\n",
      "Epoch 52, loss : 0.0259, val_loss : 1.3927, acc : 0.917\n",
      "Epoch 53, loss : 0.0148, val_loss : 0.2009, acc : 0.917\n",
      "Epoch 54, loss : 0.0203, val_loss : 1.2321, acc : 0.917\n",
      "Epoch 55, loss : 0.0137, val_loss : 0.2131, acc : 0.875\n",
      "Epoch 56, loss : 0.0216, val_loss : 1.2675, acc : 0.917\n",
      "Epoch 57, loss : 0.0144, val_loss : 0.2629, acc : 0.833\n",
      "Epoch 58, loss : 0.0231, val_loss : 1.3613, acc : 0.917\n",
      "Epoch 59, loss : 0.0153, val_loss : 0.3003, acc : 0.833\n",
      "Epoch 60, loss : 0.0223, val_loss : 1.2834, acc : 0.917\n",
      "Epoch 61, loss : 0.0114, val_loss : 0.2097, acc : 0.917\n",
      "Epoch 62, loss : 0.0136, val_loss : 0.9855, acc : 0.917\n",
      "Epoch 63, loss : 0.0049, val_loss : 0.2223, acc : 0.958\n",
      "Epoch 64, loss : 0.0099, val_loss : 0.9359, acc : 0.917\n",
      "Epoch 65, loss : 0.0034, val_loss : 0.2635, acc : 0.958\n",
      "Epoch 66, loss : 0.0052, val_loss : 0.6355, acc : 0.917\n",
      "Epoch 67, loss : 0.0023, val_loss : 0.6258, acc : 0.917\n",
      "Epoch 68, loss : 0.0018, val_loss : 0.4971, acc : 0.917\n",
      "Epoch 69, loss : 0.0017, val_loss : 0.5131, acc : 0.917\n",
      "Epoch 70, loss : 0.0017, val_loss : 0.5472, acc : 0.917\n",
      "Epoch 71, loss : 0.0016, val_loss : 0.5388, acc : 0.917\n",
      "Epoch 72, loss : 0.0016, val_loss : 0.5199, acc : 0.917\n",
      "Epoch 73, loss : 0.0015, val_loss : 0.5239, acc : 0.917\n",
      "Epoch 74, loss : 0.0015, val_loss : 0.5274, acc : 0.917\n",
      "Epoch 75, loss : 0.0015, val_loss : 0.5242, acc : 0.917\n",
      "Epoch 76, loss : 0.0015, val_loss : 0.5222, acc : 0.917\n",
      "Epoch 77, loss : 0.0015, val_loss : 0.5226, acc : 0.917\n",
      "Epoch 78, loss : 0.0015, val_loss : 0.5222, acc : 0.917\n",
      "Epoch 79, loss : 0.0015, val_loss : 0.5212, acc : 0.917\n",
      "Epoch 80, loss : 0.0014, val_loss : 0.5206, acc : 0.917\n",
      "Epoch 81, loss : 0.0014, val_loss : 0.5201, acc : 0.917\n",
      "Epoch 82, loss : 0.0014, val_loss : 0.5196, acc : 0.917\n",
      "Epoch 83, loss : 0.0014, val_loss : 0.5190, acc : 0.917\n",
      "Epoch 84, loss : 0.0014, val_loss : 0.5187, acc : 0.917\n",
      "Epoch 85, loss : 0.0014, val_loss : 0.5184, acc : 0.917\n",
      "Epoch 86, loss : 0.0014, val_loss : 0.5179, acc : 0.917\n",
      "Epoch 87, loss : 0.0014, val_loss : 0.5176, acc : 0.917\n",
      "Epoch 88, loss : 0.0013, val_loss : 0.5172, acc : 0.917\n",
      "Epoch 89, loss : 0.0013, val_loss : 0.5168, acc : 0.917\n",
      "Epoch 90, loss : 0.0013, val_loss : 0.5165, acc : 0.917\n",
      "Epoch 91, loss : 0.0013, val_loss : 0.5161, acc : 0.917\n",
      "Epoch 92, loss : 0.0013, val_loss : 0.5158, acc : 0.917\n",
      "Epoch 93, loss : 0.0013, val_loss : 0.5155, acc : 0.917\n",
      "Epoch 94, loss : 0.0013, val_loss : 0.5152, acc : 0.917\n",
      "Epoch 95, loss : 0.0013, val_loss : 0.5149, acc : 0.917\n",
      "Epoch 96, loss : 0.0012, val_loss : 0.5146, acc : 0.917\n",
      "Epoch 97, loss : 0.0012, val_loss : 0.5143, acc : 0.917\n",
      "Epoch 98, loss : 0.0012, val_loss : 0.5140, acc : 0.917\n",
      "Epoch 99, loss : 0.0012, val_loss : 0.5138, acc : 0.917\n",
      "test_acc : 0.967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "source": [
    "#### 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:3: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../term1/Sprint/train.csv\")\n",
    "X = df[[\"YearBuilt\",\"GrLivArea\"]]\n",
    "y = df[\"SalePrice\"][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[208500],\n",
       "       [181500],\n",
       "       [223500],\n",
       "       ...,\n",
       "       [266500],\n",
       "       [142125],\n",
       "       [147500]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.24107763],\n",
       "       [0.20358284],\n",
       "       [0.26190807],\n",
       "       ...,\n",
       "       [0.321622  ],\n",
       "       [0.14890293],\n",
       "       [0.15636717]])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X = mms.fit_transform(X)\n",
    "y = mms.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, loss : 22.2208, val_loss : 1.3779\n",
      "Epoch 10, loss : 0.0047, val_loss : 0.038857\n",
      "Epoch 20, loss : 0.0020, val_loss : 0.017658\n",
      "Epoch 30, loss : 0.0015, val_loss : 0.013838\n",
      "Epoch 40, loss : 0.0013, val_loss : 0.011541\n",
      "Epoch 50, loss : 0.0011, val_loss : 0.010099\n",
      "Epoch 60, loss : 0.0010, val_loss : 0.0090221\n",
      "Epoch 70, loss : 0.0009, val_loss : 0.0081651\n",
      "Epoch 80, loss : 0.0008, val_loss : 0.0075503\n",
      "Epoch 90, loss : 0.0008, val_loss : 0.007125\n"
     ]
    }
   ],
   "source": [
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch {}, loss : {:,.4f}, val_loss : {:,.5}\".format(epoch, total_loss, val_loss))"
   ]
  },
  {
   "source": [
    "#### 【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "ipykernel_launcher:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "#前処理\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\nipykernel_launcher:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Epoch 0, loss : 1008.3402, val_loss : 1.2447, acc : 0.654\n",
      "Epoch 1, loss : 76.3668, val_loss : 0.8785, acc : 0.747\n",
      "Epoch 2, loss : 55.7308, val_loss : 0.7281, acc : 0.834\n",
      "Epoch 3, loss : 40.1536, val_loss : 0.4946, acc : 0.879\n",
      "Epoch 4, loss : 33.7638, val_loss : 0.5392, acc : 0.887\n",
      "Epoch 5, loss : 32.2993, val_loss : 0.4532, acc : 0.880\n",
      "Epoch 6, loss : 29.2810, val_loss : 0.4396, acc : 0.911\n",
      "Epoch 7, loss : 24.4411, val_loss : 0.4451, acc : 0.917\n",
      "Epoch 8, loss : 24.7542, val_loss : 0.3963, acc : 0.924\n",
      "Epoch 9, loss : 24.9353, val_loss : 0.4582, acc : 0.912\n",
      "test_acc : 0.915\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "n_hidden1 = 200\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred =  tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "\n",
    "        total_loss /= batch_size\n",
    "        total_acc /= batch_size\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}