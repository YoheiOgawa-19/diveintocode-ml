{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint 機械学習フロー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.機械学習フロー\n",
    "\n",
    "\n",
    "Kaggleの Home Credit Default Risk コンペティションを題材に、機械学習の実践的な流れを学びます。特に適切な 検証 を行い、高い 汎化性能 のあるモデルを完成させることを目指します。\n",
    "\n",
    "[Home Credit Default Risk | Kaggle](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "#### 【問題1】クロスバリデーション\n",
    "事前学習期間では検証データをはじめに分割しておき、それに対して指標値を計算することで検証を行っていました。（ホールドアウト法）しかし、分割の仕方により精度は変化します。実践的には クロスバリデーション（交差検証） を行います。分割を複数回行い、それぞれに対して学習と検証を行う方法です。複数回の分割のためにscikit-learnにはKFoldクラスが用意されています。\n",
    "\n",
    "事前学習期間の課題で作成したベースラインモデルに対してKFoldクラスによるクロスバリデーションを行うコードを作成し実行してください。\n",
    "\n",
    "[sklearn.model_selection.KFold — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression  # 疑似データ作成用\n",
    "from sklearn.model_selection import train_test_split  # 疑似データ作成用\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../Week4/application_train.csv\")\n",
    "test = pd.read_csv(\"../../Week4/application_test.csv\")\n",
    "train_X = train.drop([\"TARGET\"],axis=1)\n",
    "train_y = train[\"TARGET\"]\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.drop([\"SK_ID_CURR\"],axis=1)\n",
    "test = test.drop([\"SK_ID_CURR\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train_X.dtypes[train_X.dtypes!=object].index.values\n",
    "test_num = test.dtypes[test.dtypes!=object].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104,), (104,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num.shape,test_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n",
    "       'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNT_CHILDREN                  0\n",
       "AMT_INCOME_TOTAL              0\n",
       "AMT_CREDIT                    0\n",
       "AMT_ANNUITY                   0\n",
       "AMT_GOODS_PRICE               0\n",
       "REGION_POPULATION_RELATIVE    0\n",
       "DAYS_BIRTH                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X2 = train_X[cols].fillna(0)\n",
    "train_X2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train_X2.values,train_y.values,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230633, 7), (76878, 7), (230633,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041217758872266"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pre = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test,y_pre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クロスバリデーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_X2.values\n",
    "y = train_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(X)\n",
    "scores=[]\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pre = clf.predict_proba(X_test)[:,1]\n",
    "    score = roc_auc_score(y_test,y_pre)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6063397847859013,\n",
       "  0.6061090615614468,\n",
       "  0.6020403292004952,\n",
       "  0.6087892076616084],\n",
       " 'mean0.6058195958023629')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores,\"mean{}\".format(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題2】グリッドサーチ\n",
    "これまで分類器のパラメータには触れず、デフォルトの設定を使用していました。パラメータの詳細は今後のSprintで学んでいくことになります。機械学習の前提として、パラメータは状況に応じて最適なものを選ぶ必要があります。最適なパラメータを探していくことを パラメータチューニング と呼びます。パラメータチューニングをある程度自動化する単純な方法としては グリッドサーチ があります。\n",
    "\n",
    "scikit-learnのGridSearchCVを使い、グリッドサーチを行うコードを作成してください。そして、ベースラインモデルに対して何らかしらのパラメータチューニングを行なってください。どのパラメータをチューニングするかは、使用した手法の公式ドキュメントを参考にしてください。\n",
    "\n",
    "[sklearn.model_selection.GridSearchCV — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "GridSearchCVクラスには引数としてモデル、探索範囲、さらにクロスバリデーションを何分割で行うかを与えます。クロスバリデーションの機能も含まれているため、これを使用する場合はKFoldクラスを利用する必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 7), (307511,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid)\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n",
      "0.9192451652313359\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6059724980057503,\n",
       "  0.6053849490781649,\n",
       "  0.6060261192669107,\n",
       "  0.6017049294727999,\n",
       "  0.6100858508661682],\n",
       " 'mean0.605819586194731')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(X)\n",
    "scores0=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr2.fit(X_train,y_train)\n",
    "    y_pre = lr2.predict_proba(X_test)[:,1]\n",
    "    score = roc_auc_score(y_test,y_pre)\n",
    "    scores0.append(score)\n",
    "scores2,\"mean{}\".format(np.mean(scores0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題3】Kaggle Notebooksからの調査\n",
    "KaggleのNotebooksから様々なアイデアを見つけ出して、列挙してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ハイパーパラメータのランダムサーチ　randomizedsearchCV\n",
    "- ベイズ最適化　(hyperopt,optunna,scikit-optimize)\n",
    "- 計算時間を節約するためにクロスバリデーション全てのfoldでなく  そのうち一つのfoldを使って精度を確認する。\n",
    "- foldの分け方を変えて平均を使う\n",
    "- 同じモデルの乱数シードを変えて平均をとる\n",
    "- ２つ以上のモデルを組み合わせて予測する\n",
    "- アンサンブルを行う\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題4】高い汎化性能のモデル作成\n",
    "問題3で見つけたアイデアと、独自のアイデアを組み合わせ高い汎化性能のモデル作りを進めてください。\n",
    "\n",
    "その過程として、何を行うことで、クロスバリデーションの結果がどの程度変化したかを表にまとめてください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitの数を変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 61503  61504  61505 ... 307508 307509 307510] TEST: [    0     1     2 ... 61500 61501 61502]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 61503  61504  61505 ... 123002 123003 123004]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [123005 123006 123007 ... 184504 184505 184506]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [184507 184508 184509 ... 246006 246007 246008]\n",
      "TRAIN: [     0      1      2 ... 246006 246007 246008] TEST: [246009 246010 246011 ... 307508 307509 307510]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6059724980057503,\n",
       "  0.6053849490781649,\n",
       "  0.6060261192669107,\n",
       "  0.6017049294727999,\n",
       "  0.6100858508661682],\n",
       " 'mean0.6058348693379589')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,random_state=71)\n",
    "kf.get_n_splits(X)\n",
    "scores2=[]\n",
    "lft=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr2.fit(X_train,y_train)\n",
    "    y_pre2 = lr2.predict_proba(X_test)[:,1]\n",
    "    score = roc_auc_score(y_test,y_pre2)\n",
    "    scores2.append(score)\n",
    "    lft.append(y_pre2)\n",
    "scores2,\"mean{}\".format(np.mean(scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 違うモデルを試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6207453587857042,\n",
       "  0.6187088857664038,\n",
       "  0.6096555763208752,\n",
       "  0.6162109540320113],\n",
       " 'mean0.6163301937262486')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=4,random_state=71)\n",
    "kf.get_n_splits(X)\n",
    "scores3=[]\n",
    "rant=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    rfc.fit(X_train,y_train)\n",
    "    y_pre3 = rfc.predict_proba(X_test)[:,1]\n",
    "    score = roc_auc_score(y_test,y_pre3)\n",
    "    scores3.append(score)\n",
    "    rant.append(y_pre3)\n",
    "scores3,\"mean{}\".format(np.mean(scores3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "[17:30:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:31:00] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6565378127614647,\n",
       "  0.6555051190798276,\n",
       "  0.6501837187469788,\n",
       "  0.6554634097356806],\n",
       " 'mean0.6544225150809879')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xmo = xgb.XGBClassifier()\n",
    "kf = KFold(n_splits=4,random_state=71)\n",
    "kf.get_n_splits(X)\n",
    "scores4=[]\n",
    "xmot=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    xmo.fit(X_train,y_train)\n",
    "    y_pre4 = xmo.predict_proba(X_test)[:,1]\n",
    "    score = roc_auc_score(y_test,y_pre4)\n",
    "    scores4.append(score)\n",
    "    xmot.append(y_pre4)\n",
    "scores4,\"mean{}\".format(np.mean(scores4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エクストラツリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.603602517303539,\n",
       "  0.6034187535895288,\n",
       "  0.5946225975297057,\n",
       "  0.6039120433613492],\n",
       " 'mean0.6013889779460306')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = ExtraTreesClassifier()\n",
    "kf = KFold(n_splits=4,random_state=71)\n",
    "kf.get_n_splits(X)\n",
    "scores6=[]\n",
    "ett = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    et.fit(X_train,y_train)\n",
    "    y_pre6 = et.predict_proba(X_test)[:,1]\n",
    "    score6 = roc_auc_score(y_test,y_pre6)\n",
    "    scores6.append(score6)\n",
    "    ett.append(y_pre6)\n",
    "scores6,\"mean{}\".format(np.mean(scores6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スタッキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48744, 7), (307511, 7))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = test[cols].fillna(0).values\n",
    "test_x.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# パラメータ\n",
    "ntrain = train.shape[0] # 891\n",
    "ntest = test.shape[0] # 418\n",
    "SEED = 0\n",
    "NFOLDS = 5 # 5分割\n",
    "kf = KFold(n_splits= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Sclearn分類機を拡張\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "\n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train,y_train)): # NFOLDS回まわる\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi = SklearnHelper(clf=LogisticRegression,params={'C' : 0.001 })\n",
    "ran = SklearnHelper(clf=RandomForestClassifier,params={'n_jobs':-1, 'max_depth': 6,})\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier,params={'max_depth': 5})\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier,params={'n_estimators': 500})\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier,params={'max_depth': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logi_train,logi_test = get_oof(logi,X,y,test_x)\n",
    "ran_train,ran_test = get_oof(ran,X,y,test_x)\n",
    "gb_train,gb_test = get_oof(gb,X,y,test_x)\n",
    "ada_train,ada_test = get_oof(ada,X,y,test_x)\n",
    "et_train,et_test = get_oof(et,X,y,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01057125 0.03914087 0.17121599 0.13723603 0.21867806 0.08960133\n",
      " 0.33355648]\n",
      "[0.01349136 0.0529016  0.17151891 0.18503347 0.2068336  0.12543991\n",
      " 0.24478115]\n",
      "[0.02  0.038 0.204 0.204 0.164 0.248 0.122]\n",
      "[0.01819274 0.01596718 0.1104713  0.05911038 0.14122406 0.13360715\n",
      " 0.52142719]\n"
     ]
    }
   ],
   "source": [
    "#logi_feature = logi.feature_importances(X,y)\n",
    "ran_feature = ran.feature_importances(X,y)\n",
    "gb_feature = gb.feature_importances(X,y)\n",
    "ada_feature = ada.feature_importances(X,y)\n",
    "et_feature = et.feature_importances(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_feature = [0.01057125, 0.03914087, 0.17121599, 0.13723603, 0.21867806, 0.08960133, 0.33355648]\n",
    "gb_feature = [0.01349136, 0.0529016,  0.17151891, 0.18503347, 0.2068336,  0.12543991, 0.24478115]\n",
    "ada_feature = [0.02,  0.038, 0.204, 0.204, 0.164, 0.248, 0.122]\n",
    "et_feature = [0.01819274, 0.01596718, 0.1104713,  0.05911038, 0.14122406, 0.13360715, 0.52142719]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_predictions_train.shape :  (307511, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>ExtraTrees</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>GradientBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  ExtraTrees  AdaBoost  GradientBoost\n",
       "0           0.0         0.0       0.0            0.0\n",
       "1           0.0         0.0       0.0            0.0\n",
       "2           0.0         0.0       0.0            0.0\n",
       "3           0.0         0.0       0.0            0.0\n",
       "4           0.0         0.0       0.0            0.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame( \n",
    "    {'RandomForest': ran_train.ravel(),\n",
    "     'ExtraTrees': et_train.ravel(),\n",
    "     'AdaBoost': ada_train.ravel(),\n",
    "      'GradientBoost': gb_train.ravel()\n",
    "    })\n",
    "print('base_predictions_train.shape : ', base_predictions_train.shape)\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (307511, 5)\n",
      "x_test.shape :  (48744, 5)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate(( et_train, ran_train, ada_train, gb_train,logi_train), axis=1)\n",
    "x_test = np.concatenate(( et_test, ran_test, ada_test, gb_test,logi_test), axis=1)\n",
    "print('x_train.shape : ', x_train.shape)\n",
    "print('x_test.shape : ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "[16:34:52] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n",
      "[16:35:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n",
      "[16:36:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n",
      "[16:37:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5001195928210687,\n",
       "  0.4998723567204187,\n",
       "  0.5000915977754025,\n",
       "  0.5000392588876704],\n",
       " 'mean0.50003070155114')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmo = xgb.XGBClassifier(n_estimators= 2000,max_depth= 8,)\n",
    "kf = KFold(n_splits=4,random_state=71)\n",
    "kf.get_n_splits(x_train)\n",
    "scores5=[]\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train1, X_test1 = x_train[train_index], x_train[test_index]\n",
    "    y_train1, y_test1 = y[train_index], y[test_index]\n",
    "    \n",
    "    xmo.fit(X_train1,y_train1)\n",
    "    y_pre5 = xmo.predict_proba(X_test1)[:,1]\n",
    "    score1 = roc_auc_score(y_test1,y_pre5)\n",
    "    scores5.append(score1)\n",
    "scores5,\"mean{}\".format(np.mean(scores5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5と値が下がってしまった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スタッキングその２"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それぞれのモデルの予測値を結合させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e=lft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511,)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_all = np.concatenate([a,b,c,d,e])\n",
    "lf_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6057810870956006"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y,lf_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = rant\n",
    "ran_all = np.concatenate([a,b,c,d])\n",
    "ran_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6163022051896226"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y,ran_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511,)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = xmot\n",
    "xg_all = np.concatenate([a,b,c,d])\n",
    "xg_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6543636154994501"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y,xg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511,)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c,d = ett\n",
    "et_all = np.concatenate([a,b,c,d])\n",
    "et_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6013854872364193"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y,et_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全体 :  (307511, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ロジスティック回帰</th>\n",
       "      <th>ランダムフォレスト</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>エクストラツリー</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.176915</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048792</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.051697</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069418</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.034552</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058801</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.059227</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.057191</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ロジスティック回帰  ランダムフォレスト   XGBoost  エクストラツリー\n",
       "0   0.155668       0.22  0.176915      0.27\n",
       "1   0.048792       0.04  0.051697      0.05\n",
       "2   0.069418       0.02  0.034552      0.00\n",
       "3   0.058801       0.08  0.059227      0.08\n",
       "4   0.043945       0.03  0.057191      0.03"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta = pd.DataFrame( \n",
    "    {'ロジスティック回帰': lf_all.ravel(),\n",
    "     'ランダムフォレスト': ran_all.ravel(),\n",
    "     'XGBoost': xg_all.ravel(),\n",
    "     'エクストラツリー': et_all.ravel(),})\n",
    "print('全体 : ', sta.shape)\n",
    "sta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4つの予測値からxgboostにて一つの予測値を出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = sta.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n",
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 76878  76879  76880 ... 307508 307509 307510] TEST: [    0     1     2 ... 76875 76876 76877]\n",
      "[21:01:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 76878  76879  76880 ... 153753 153754 153755]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:01:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [153756 153757 153758 ... 230631 230632 230633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:02:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "TRAIN: [     0      1      2 ... 230631 230632 230633] TEST: [230634 230635 230636 ... 307508 307509 307510]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:02:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6477740959758902,\n",
       "  0.6483415889949594,\n",
       "  0.6436278273959045,\n",
       "  0.6502155792589552],\n",
       " 'mean0.6474897729064274')"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=4,random_state=71)\n",
    "kf.get_n_splits(X)\n",
    "scores7=[]\n",
    "end=[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X1[train_index], X1[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    xmo.fit(X_train,y_train)\n",
    "    y_pre7 = xmo.predict_proba(X_test)[:,1]\n",
    "    score7 = roc_auc_score(y_test,y_pre7)\n",
    "    scores7.append(score7)\n",
    "    end.append(y_pre7)\n",
    "scores7,\"mean{}\".format(np.mean(scores7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base :  (1, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ロジスティック回帰クロスバリデーション（ノーマル）</th>\n",
       "      <th>ロジスティック回帰グリッドサーチ後</th>\n",
       "      <th>ロジスティック回帰分割数変更</th>\n",
       "      <th>ランダムフォレスト</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>エクストラツリー</th>\n",
       "      <th>スタッキング</th>\n",
       "      <th>スタッキングその2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60582</td>\n",
       "      <td>0.60582</td>\n",
       "      <td>0.605835</td>\n",
       "      <td>0.61633</td>\n",
       "      <td>0.654423</td>\n",
       "      <td>0.601389</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.64749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ロジスティック回帰クロスバリデーション（ノーマル）  ロジスティック回帰グリッドサーチ後  ロジスティック回帰分割数変更  ランダムフォレスト  \\\n",
       "0                    0.60582            0.60582        0.605835    0.61633   \n",
       "\n",
       "    XGBoost  エクストラツリー    スタッキング  スタッキングその2  \n",
       "0  0.654423  0.601389  0.500031    0.64749  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = pd.DataFrame( \n",
    "    {'ロジスティック回帰クロスバリデーション（ノーマル）': np.mean(scores).ravel(),\n",
    "     'ロジスティック回帰グリッドサーチ後': np.mean(scores0).ravel(),\n",
    "     'ロジスティック回帰分割数変更': np.mean(scores2).ravel(),\n",
    "     'ランダムフォレスト': np.mean(scores3).ravel(),\n",
    "     'XGBoost': np.mean(scores4).ravel(),\n",
    "     'エクストラツリー': np.mean(scores6).ravel(),\n",
    "     'スタッキング': np.mean(scores5).ravel(),\n",
    "     'スタッキングその2': np.mean(scores7).ravel(),\n",
    "    })\n",
    "print('base : ', base.shape)\n",
    "base.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "普通にxgboostを試した方が良い値であった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題5】最終的なモデルの選定\n",
    "最終的にこれは良いというモデルを選び、推定した結果をKaggleに提出してスコアを確認してください。どういったアイデアを取り入れ、どの程度のスコアになったかを記載してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グリッドサーチや、クロスバリデーションの分割数を変えたり、アンサンブルのスタッキングという方法で推測したが、XGBoostを普通に試したモデルが一番roc評価が高くなった。それでも大きな値の変化はなく、モデルチューニングや、モデル変更ではなく前処理が一番重要だということが今回の課題を通じてわかった。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前処理でターゲットエンコーディングをしたxgboostをkaggleに提出することとした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../Week4/application_train.csv\")\n",
    "test = pd.read_csv(\"../../Week4/application_test.csv\")\n",
    "train_X = train.drop([\"TARGET\"],axis=1)\n",
    "train_y = train[\"TARGET\"]\n",
    "test_y = pd.read_csv(\"../../Week4/application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = train_X[train_X==object].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ob:\n",
    "    data_tmp = pd.DataFrame({i:train_X[i],\"target\":train_y})\n",
    "    target_mean = data_tmp.groupby(i)[\"target\"].mean()\n",
    "    test[i] = test[i].map(target_mean)\n",
    "    \n",
    "    tmp = np.repeat(np.nan,train_X.shape[0])\n",
    "    \n",
    "    kf = KFold(n_splits=4,shuffle=True,random_state=72)\n",
    "    for idx_1,idx_2 in kf.split(train_X):\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(i)[\"target\"].mean()\n",
    "        tmp[idx_2]  = train_X[i].iloc[idx_2].map(target_mean)\n",
    "        \n",
    "    train_X[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a100/.pyenv/versions/anaconda3-5.1.0/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:33:32] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=20,random_state=71)\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred = model.predict_proba(test)[:,1]\n",
    "submission = pd.DataFrame({\"SK_ID_CURR\":test_y[\"SK_ID_CURR\"],\n",
    "                           \"TARGET\":pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"sub10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kaggleで0.71883という点数となった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
